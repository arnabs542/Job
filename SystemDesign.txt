* https://www.evernote.com/shard/s576/sh/7e58b450-1abe-43a8-bf82-fbf07f1db13c/049802174415b418a2e65f75b744ab72

* https://www.hiredintech.com/classrooms/system-design/lesson/55

* scalability. e.g. DB(Master-slave), sharding, Cache(Redis, Memcached), Load-balancing.

---------------------------
Notes
---------------------------
* Scenario
  - daily active users (DAU)
  - average qps
  - peak qps, assume
  - read/write qps to decide how many servers to use
  - feature / interfaces

* Service
  - Error Handling / Retry
  - Alarm

* Storage
  - schema, sql, nosql, file system, memory
  - read, write

* Scale
  - special cases
  - sharding / optimize

---------------------------
News Feed
---------------------------
* scenario
    - dau: daily active users
    - qps(query per second)
      1. relation with dau * freq / daily seconds
      2. relation with server.
         1 webserver 1k QPS, 1 sql 1k, 1 nosql 10k, 1 memcased nosql 1m qps
      3. peak qps = 3 * average qps
      4. fast growing product
    - feature / interfaces
    - Enumerate features
        - Register / Login
        - profile display
        - Post tweet
        - news feed
        - follow / unfollow user
        - Search
        - upload image / video
      Then Sort features
        1. Post tweet 2. Timeline 3. News Feed 4. Follow 5. Register

* service
  - 将scenario 里的features进行过一遍然后按照逻辑进行归类

  1. User Service
    - Register
    - Login

  2. Tweet Service
    - Post tweet
    - News Feed

  3. Media Service
    - Upload Image
    - Upload Video

  4. Friendship Service
     - Follow / UnFollow

* storage
  - schema
  1. User Service (SQL)
     - User table
      - id: int
      - username: string
      - email: string
      - password: string

  2. Tweet Service (NoSQL)
     - Tweet Table
       - id: int
       - user_id: foreign key
       - content: string
       - created_at: timestamp

  3. Media Service (File System)

  4. Friendship Service (SQL / NoSQL)
     - Friendship
      - id: int
      - from_user_id: foreign key
      - to_user_id: foreign key

* News Feed，
  - Pull: 读慢
    - 根据上次刷新时间和现在时间，获取所有followers的tweets
    1. request to get news feed
    2. get followers
    3. get tweets based on user id, time
    4. merge and return
    Notes: 慢，synchronized call

  - Push: 写慢
    - 每个用户建个news feed table， 当用户发tweet后，推送到table里，查看时从中读取。
    - News Feed Table
      - id: int
      - owner_id: foreign key
      - tweet_id: foreign key
      - created_at: timestamp
    - read 1DB, DB write可以异步执行，无需等待。但用户量可能很大
    - workflow
      1. Post a new tweet
      2. Insert to tweet table
      3. get followers
      4. fanout to followers news feed

* scale
  - Cache
    1. Pull: cache 用户的news feed

  - sharding
  - optimze
  - failure recover
  - special case

* Pull + Push
  - push for normal users with fewer followers
  - pull for star with lots of followers
  - 可能出现摇摆问题，吊粉

* Follow / Unfollow
  - follow， 异步将timeline合并到news feed。Unfollow 则异步移除。异步好处，用户立马得到反馈，坏处是news feed有延迟

* Like
  - Like Table
    - id: int
    - user_id: Foreign Key
    - tweet_id: Foreign Key
    - Created_at timestamp
  - Tweet Table: de-normalized
    - like_nums: int
    - comment_nums: int
    - retweet_nums

---------------------------
Database
---------------------------
* QPS
  - MySQl 1k
  - 硬盘型NoSQL数据库 MongoDB/Cassandra 10k
  - 内存型数据库 Redis/Memcached(单机) 100k - 1m QPS，
  - Redis单机支持数据持久化，Memcached可分步不支持数据持久化.Redis 读写都很快的cache-through db.

* 思考读多于写和是反过来，读的多一定要用cache来优化

* cache delete ; database set

* Design User System
  * Scenario
    - register, login, search, profile modification, friendship
    - DAU 100M
    - For register, login, profile: 100M * 0.1 / 24*3600 = 100 QPS, Peak 3 * 100 = 300 QPS. For search, 100M * 100 / 24 * 3600 = 100k, Peak 300K

  * Service
    - AuthService (Login, Register)
      - Login uses session table. 作为cookie值返回browser, then request to website including cookie. Logout will remove session from table or get expired
      - session table 推荐存数据库而不是内存
    - UserService
      - search
    - FriendshipService
      - mutual friendship: smaller_user_id, bigger_user_id or from_user_id, to_user_id

  * Storage
    - Session Table // for login,可允许1个user多个session key， 说明在不同浏览器上登录
      id: int
      user_id: int
      session_key: string
      expire_time: timestamp

    - FriendShip Table
      id: int
      from_user_id: id
      to_user_id: id

* SQL or NoSQL
  - SQL is mature and support much more things than NoSQL. NoSQL support higher QPS.
  - NoSQL doesn't support transactional

* Cassandra
  - insert(row_key, column_key, value)
  - row_key, column_key, value决定存在哪个database上
  - row_key: hash_key, for routing, sharding purpose，can't range query
  - column_key: sorted, support range query

* Scale: Single Point Failure
  - Sharding: 拆分数据存储到不同数据库
      Vertical Sharding：tables存入不同数据库
      Horizontal Sharding: put 1 table into diff dbs. Add new db machine, how to resolve data migration issue? consistent hashing.
  - Replica

---------------------------
Crawler & Typehead
---------------------------
* Web Crawler
  * Scenario
    1. how many web pages per second
    2. how long
    3. how large 10k average size of a webpage

  * Service
    - Crawler
    - Storage Service
    - Task Service

  * Storage
    - db to store tasks (queue e.g. sqs is small)
      id: int
      url: string
      state: enum
      priority: int
      available_time: timestamp // 控制抓取频率

  * Scale
    - single thread -> multi thread
    - multi thread -> distributed
    - queue -> table
    - slow select -> db horizontal sharding
    - crawl failure -> exponetial retry

    - bigTable to store web pages

* Single-threaded web crawler
  - Producer Consumer Pattern, buffer
  - bfs, remove()-> store page -> extract urls -> offer()

* multiple threaded
  - multiple crawler + 1 URL queue
  - 3 approaches to resolve multi-threaded buffer write
    1. sleep
    2. condition variable
    3. semaphore
  - context switch cost, port number limitation, network bottleneck

* distributed web scrawler
  - URL queue need to be very big: task table(url, state, priority, available_time )
  - Service: CrawLer, TaskService, StorageServices
  - Storage: db task table, BigTable to store web pages
  - Optimization:
    1. distributed task tables, horizontal sharding
    2. set available_time based on update faliture
    3. dead cycle, use quota

* Typeahead: 提示关键词
  - prefix -> top hot search keywords

* Scenario
  - DAU 500m, QPS: 138k, Peak: QPS*3

* Service
  - QueryService: trie (in memory)
  - DataCollectionService：
    1. table keyword, hit_count -> prefix, keywords

* distributed trie
  - can't distribute by a, b, c... cause its not averaged. Calculate hash and matain map from hash to distributed server

* log data service
  - modify trie periodically e.g. 10 min, then switch to new trie
  - probabilistic logging: 每1000个log一次

---------------------------
Distributed File System (Goole File System)
---------------------------
* Peer To Peer vs Master Slave

* GFS vs HDFS
  - GFS is not open source, HDFS is. Other is the same.

* Scenario
  - read/write file
  - how large: 1000T

* Service
  - client, servers
  - servers communication
    - peer to peer: e.g. BitComet, Cassandra
    - master, slave: partition, naster扮演管理者位置

* Storage:
  - Peer 2 Peer: Cassandra, redundency
  - Master Slave: easy design. 挂了就重启

* Save a small file in one machine
  - file and meta data. Save meta data separetely
  - windows 连续化存储， linux 碎片化存储
  - meta data has index which includes many blocks(1KB) and each block point to the disk address that the file is stored

* Save a large file in one machine
  - index includes chunk(64MB) instead of block to reduce size

* Save extra-large file in several machines
  - Master: matain metadata, map of chunk and chunk server
  - Slave: store chunk
  - Master include index (chunks server) info, Salve ChunkServer includes chunk info

* Write (client, server(master, slave))
  - 拆分写入DFS: 断点续传，以chunk为单位
  - client -> master: 写chunk1
    master -> client: 回chunkserver list
    client -> slave: 写， 从slave1，2，3中选个队长1写进去，然后slave1再写给其他slave
  - Client: Database, WebServer, Server: GFS, client and server是相对的概念

* Update
  - write new data to fs, change the index in master to the new data address. Leave the old data there.

* Read
  - 拆分的部分从master index(chunk list)中知道，然后共同读取

* Scale
  + one master(90% cases) 不够
  + chunk坏了
    - checksum(MD5, SHA1, SHA256, SHA512), add checksum to the end of chunk. When read data, calculate checksum and compare the one at the end.
    - 周期性检查，读的时候检查checksum
    - Recover: ask master
  + Replica
    - 3份: 2个近的，1个远的
  + heartbeat,验证chunkserver在不在

------------------------------------
Consistent Hashing & Design Tiny Url
------------------------------------
* Consistent Hashing by Horizontal Sharding
  - mod 360, 分配给n台机器，每个机器负责一段区间。n从2到3，只有1/3的数据移动。
  - 目的是减小数据的移动
  - 缺陷：数据不均匀(e.g. n从3到4)，迁移压力大

* Consistent hashing
  - hash func(数据) return (mod 2^64-1) 个点中一个， 把机器和数据放在这些点上。数据算好点后，顺时针找到的第一个机器(虚拟节点)为要存的。
  - 1个机器分成1000个虚拟节点，平均分布在环上。1000个虚拟节点有1000个不连续的区间

* Back up vs Replica
  - backup 是周期的，不是实时的
  - replica是实时的

* MySql Replica: Master - Slave
  - Master: write and read
  - Slave: read, sync from Master (Write Ahead Log, 执行master之前的操作，所以造成一定延迟)
  - Master 挂了，promote Slave.没有同步过来的数据可能丢失。

* NoSQL Replica
  - Consistent Hashing 环上顺时针存3份
  - include sharding and replica (优点)

* Design Tiny URL
  * Scenario: 功能，需求，QPS，存储容量
    - redirect
    - 产生，点击Tiny URL， 存储量

  * Servie
    - URLService
      - shortToLong
      - longToShort

  * Storage
    - transaction？ no
    - 丰富的query？a little bit
    - scalability?
    - QPS?
    - Sequential id?

* Long URL to 6 digit url
  - 随机转化：随机生成短的没有用过的url存数据库。随着短网址越来越多，生成速度效率低。
  - 进制转换：依赖全局自动增加iD，转化成6位62进制的short url。62^6=57B=570亿。效率高，但依赖于全局的自增ID。有安全缺陷，容易被人猜

* Scale
  - cache
  - 地理位置信息提速：DNS解析位置
  - honrizontal sharding: what sharding key?
    1. Long URL
       long  -> short: easy
       short -> long: broadcast all database
    2. ID
       short -> long: short url to ID, use ID to find db
       long  -> short: broadcast to all servers

------------------------------------
Database System Design (Google Big Table)
------------------------------------
* File System vs Dabase System
  - FS includes     simple file read, write ops.
  - DB provides complex queries, 数据库建立在文件系统之上

* 硬盘上整理顺序，用external sort, which includes split and k merge sort

* Big Table： 基于 File System
  - Bigtable: Google, HBase: Yahoo, Cassandra:Facebook, DynamoDB: Amazon
  - Write: Update appends new data, 时常进行排序整理解决read的二分查询问题。优化方法是有有序块和无序块, 无序块可以放到memory里sort成有序 list，当大到一定程度，将其在硬盘上整理成有序块。

  Memory: file0 Address0, file1 Address1, sorted list of 无序块
  Disk: file0 有序, file1 有序

  Write:
    1. write key
    2. append key to sorted list (内存排序，最后1次写入)
    3. serialize to disk file1 （write ahead log to avoid 内存挂了）

  - Read: Add index and bloom filter in memory to map from key to address pointer for each unsorted block (sorted block不需要). All index & bloomfilter for all unsorted blocks are stored in the memory.

  Memory: file 0 Address0, Index0 bloomfilter0, file1 Address1, Index1 bloomfilter1, Sorted List of 无序块
  Disk: File 0 (index0, Bloomfilter0), File 1(index1, Bloomfilter1)

  Read:
    1. read key
    2. Read key from sorted list of 无序块 in memory
    3. If not, check bloom filter for each file has this key
    4. use index to find the value for the key

** Bloom Filter
  - 高效查找key是否在Sorted Strings里，解决大数据量情况的元素判定。可以替代set
  - False is always false, true may be true。所以有一定误判率
  - 应用： 黑名单，爬虫重复url检测，字典纠错，磁盘文件检测

* Skip list
  - matain all new write data in memory

* SSTable
  - unit that stored in the disk

* Distributed
  - master slave
  - Read: key -> consistent hash returns slave server -> ask slave server in memory 1. Check Memory Skip List 2. Chech Bloom Filter 3. Check index, then find value in Sstable in Disk
  - Write: write key -> consistent hashmap retunr slave -server -> ask slave server in memory 1. write data into memory 2. write ahead log 3.skip list 满了写进disk

* Distributed Lock
  - ZooKeeper

------------------------------------
Map Reduce
------------------------------------
* 10G才算大数据

* 不提倡用hashmap，因为大数据处理。多台机器的for循环，用map reduce

* 分布式计算框架
  - 1. input
    2. split：尽量平均的分配到map 机器上
    3. map：
      - 不用hashmap统计，e.g. a : 1, b : 1, a : 1, not a : 2 因为数据量可能很大。
      - map机器数远大于reduce的机器
      - key 一般用处不大，values重要
    4. 传输汇总：external sort, merge k sorted list。一定量后传输
    5. reduce：key， value。
    6. output
  - map，reduce机器数 e.g. 10T, 1000map, 1000reduce
  - 机器越多越好？每个处理时间短，但启动时间长
  - 不考虑启动时间，reduce机器越多越好？上限是key的数目

* 计算文章的单词数
  - Map:拆分单词, 数据放硬盘不是内存，按文章行数分。
  - Reduce: 汇总单词，按key来分。
  - 传输汇总：Partition and Sort(硬盘外排序) -> Fetch and Merge Sort(k merge sort)

* Inverted Index
  - Input: 文章Id:单词组
  - Map: key 文章id， value content of the line
  - Reduce: key 单词， value 文章Id.

* Anagram
  - reduce : key sorted word， value list of words

* Notes
  - Reducer 一个机器key特别大怎么办？加个random后缀，类似shard key
  - input和output放哪？ GFS


------------------------------------
Message System & Rate Limiter
------------------------------------
* Message System
  1. Scenarios
    - Features
       - login /regsiter
       - contact （follow）
       - 相互发消息
       - 群聊
       - 用户在线
       - 历史记录
       - 多台登陆
    - 100m daily users, QPS 100*20/86400 ~ 20k, peak 20k * 5 = 100k, 读写都高

  2. Services
     - Message Service
     - Real-time Service

  3. Storage
     - Message Table: Use NoSQL, 数据量大，无需修改。按thread id primary key。
       - id : int
       - from_user_id: int
       - to_user_id: int
       - content: string
       - created_at: timestamp

       Improve to
       - id : int
       - from_user_id: int
       - thread_id: int
       - content: string
       - created_at: timestamp

     - Thread Table: inbox has threads, thread has messages。SQL, index by ownerId + ThreadId, ownerId + updated Time
       - id: int
       - participant_ids: text
       - created_at: timestamp
       - updated_at: timestamp (排序用)

       Improve to
       - owner_id: int (谁的thread)
       - thread_id: int
       - participant_ids: text
       - is_muted bool
       - nickname: string
       - created_at: timestamp
       - updated_at: timestamp (排序用)

  4. Work solution
     - 发消息
          1. client 把消息和接受者信息发给server
          2. server创建message, and thread if necessary
     - 接受消息
          1. Poller： 每隔10s polling一下

  5. Scale
    - 接受消息太慢
      - socket: tcp长连接，保持连接。与http区别是http只能client to server. Socket是双向的。这样子就enable push service from server to client 了。
      - android GCM and IOS APNS notification service。关闭app后，用app pull + GCM/APNS.
      - Workflow
        1. A client开机，产生定时的pull request to webserver. Also ask push server ip from webserver。
        2. webserver gives push server ip. A和push server保持长连接。
        3. B client给A发信息。信息经过webserver, push server 到达A

       - Push service 本身可以不发送消息，而是提醒client去pull消息。

    - 群聊
      - 问题是大家在不在线。加个channel service btw push service and message service。用于管理谁在线，该推送。数据存在memory里.对于没有在线的人，他们登陆的时候通过pull得到历史消息。

    - 在线状态
      - client 通过heartbeat主动汇报，用户知道我哪些好友在线也是pull。
      - push取决于client用户端网络情况，情况复杂不可控因而不稳定。所以对于real time要求不高，用pull，一方面也能汇报在不在

* Rate Limiter
  - limit requests.限制数据库的读写操作。
  - 用memcached实现，因为不需要数据持久化。
    1. 用event + feature + timestamp作为memechached的key
    2. 记录一次访问，memcached.increament(key, ttl=60s)
    3. 查询是否限制。for t in 0 - 59 do, key = event + feature + (current_timestamp -t), sum+=memcached.get(key, default = 0)
    4. check sum with limitation

  - scale
    1. 分级存储。之前是按秒来存储，可以按分钟，小时进行存储。
    2. 允许一定的误差

* Design Datadog
    - 对老数据进行瘦身(retention)e.g. 对于老的request num，不按秒存，而是按小时存

------------------------------------
Location Based Service
------------------------------------
* Uber
  - RingPop: distributed framework
  - TChannel: RPC
  - Google S2: 地理信息存储与查询算法
  - Riak: DynamoDB 开源

* Design Uber
  - Scenario
    - Feature
      - driver report locations
      - rider request Uber
      - driver cancel / deny request
      - rider cancel request
      - driver pick up and start
      - driver drop off and end trip

    - Driver 20w, QPS 200k / 4 = 50k, Peak 150k, write

  - Service
    - GeoService (Redis)
      - Driver save location
      - Rider, find drivers nearby

    - DispatchService (SQL)
      - Driver report locations per 4s, Get matched rider.
      - Rider request Uber, return matched driver

  - Storage
    - Trip Table
       tripId: int
       dirverId: int;
       riderId: int,
       Latitude, Longitude: double
       status: int
       createdAt: timestamp

    - Location Table: 大量的写操作
        driverId: int
        updatedAt: timestamps
        Latitude, Longitude: double
        geohash: String // select * from location where geohash LIKE '9q9hv%'

    - Driver Table

  - find nearby driver by geohash
    1. 使用geohash。 (lat, Ing) -> geohash -> [driver1, driver2 ...], 先查6位的geohash找0.6公里以内的， 如果没有查5位 2.4公里以内的。分级查询 如果匹配成功， 查询 Driver 位置

  - Workflow
    1. rider request, server create trip, return trip_id, pulling per seonds
    2. server find nearby driver, write to trip, change status and update driver table
    3. driver report location
    4. Driver accepts request. Update driver, trip table status info. Rider see driver info
    5. Driver deny request, update Driver, Trip status info. Repeat 2.

  - Scale
    - location 按城市sharding

* Google S2
  - Hilbwer Curve
  - 将地址空间映射到2^64的整数。空间上比较接近的2个点，对应的整数也比较接近

* Geohash
  - 将整体地图划分块状，hash长度越长越精确
  - 先经后纬，经纬交替，二分

* Redis
  - NoSQL, 支持数据持久化
  - 原生支持list,set 等结构
  - 读写速度接近内存访问速度 > 100k QPS

* Geo Fence
  - 圈定区域位置。
  - 两级Fence查询， 先找到城市，再找到airport































