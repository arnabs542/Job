* https://www.evernote.com/shard/s576/sh/7e58b450-1abe-43a8-bf82-fbf07f1db13c/049802174415b418a2e65f75b744ab72

* https://www.hiredintech.com/classrooms/system-design/lesson/55

* scalability. e.g. DB(Master-slave), sharding, Cache(Redis, Memcached), Load-balancing.


---------------------------
News Feed
---------------------------
* scenario
    - dau: daily active users
    - qps(query per second)
      1. relation with dau * freq / daily seconds
      2. relation with server.
         1 webserver 1k QPS, 1 sql 1k, 1 nosql 10k, 1 memcased nosql 1m qps

* service
  - User Service
  - Tweet Service
  - Media Service
  - Friendship Service

* storage
  - schema
  - sql: User Service, Friendship Service
  - nosql: Tweet Service
  - file system: Media Service, e.g S3

* scale
  - sharding
  - optimze
  - special case

* Pull
  - slow for read. Improve by cache

* Push - insync for push to followers
  - slow for write cause add user/follower relation into tables, but can do insync for post tweet

* Pull + Push
  - push for normal users with fewer followers
  - pull for star with lots of followers
  - 可能出现摇摆问题，吊粉

---------------------------
Database
---------------------------
* QPS
  - MySQl 1k
  - 硬盘型NoSQL数据库 MongoDB/Cassandra 10k
  - 内存型数据库 Redis/Memcached(单机) 100k - 1m QPS，
  - Redis支持数据持久化，Memcached不支持数据持久化.Redis 读写都很快的cache-through db.

* 思考读多于写和是反过来

* cache delete ; database set

* Design User System 
  - AuthService
    - Session table : login session_key 作为cookie值返回browser, then request to website including cookie. Logout will remove session from table.
    - session table 推荐存数据库而不是内存
  - UserService
  - FriendshipService
    - mutual friendship: smaller_user_id, bigger_user_id or from_user_id, to_user_id 

* SQL or NoSQL
  - SQL is mature and support much more things than NoSQL. NoSQL support higher QPS.
  - NoSQL doesn't support transactional

* Cassandra
  - insert(row_key, column_key, value)
  - row_key, column_key, value决定存在哪个database上
  - row_key: hash_key, for routing, sharding purpose，can't range query
  - column_key: sorted, support range query
  - value: 

* Scale: Single Point Failure
  - Sharding: 拆分数据存储到不同数据库
      Vertical Sharding：tables存入不同数据库
      Horizontal Sharding: put 1 table into diff dbs. Add new db machine, how to resolve data migration issue? consistent hashing.
  - Replica

---------------------------
Crawler & Typehead
---------------------------
* Web Crawler

* Scenario 
  1. how many web pages per second
  2. how long 
  3. how large 10k average size of a webpage

* Single-threaded web crawler
  - Producer Consumer Pattern, buffer 
  - bfs, remove()-> store page -> extract urls -> offer()

* multiple threaded 
  - multiple crawler + 1 URL queue
  - 3 approaches to resolve multi-threaded buffer write
    1. sleep
    2. condition variable 
    3. semaphore
  - context switch cost, port number limitation, network bottleneck

* distributed web scrawler
  - URL queue need to be very big: task table(url, state, priority, available_time )
  - Service: CrawLer, TaskService, StorageServices
  - Storage: db task table, BigTable to store web pages
  - Optimization: 
    1. distributed task tables, horizontal sharding 
    2. set available_time based on update faliture
    3. dead cycle, use quota

* Typeahead: 提示关键词
  - prefix -> top hot search keywords

* Scenario 
  - DAU 500m, QPS: 138k, Peak: QPS*3

* Service
  - QueryService: trie (in memory)
  - DataCollectionService：
    1. table keyword, hit_count -> prefix, keywords  

* distributed trie
  - can't distribute by a, b, c... cause its not averaged. Calculate hash and matain map from hash to distributed server

* log data service
  - modify trie periodically e.g. 10 min, then switch to new trie
  - probabilistic logging: 每1000个log一次

---------------------------
Distributed File System (Goole File System)
---------------------------
* GFS vs HDFS
  - GFS is not open source, HDFS is. Other is the same.

* Scenario 
  - read/write file
  - how large: 1000T

* Service 
  - client, servers
  - servers communication
    - peer to peer: e.g. BitComet, Cassandra 
    - master, slave: partition, naster扮演管理者位置

* Storage:
  - Peer 2 Peer: Cassandra, redundency
  - Master Slave: easy design. 挂了就重启

* Save a small file in one machine
  - file and meta data. Save meta data separetely 
  - windows 连续化存储， linux 碎片化存储
  - meta data has index which includes many blocks(1KB) and each block point to the disk address that the file is stored 

* Save a large file in one machine
  - index includes chunk(64MB) instead of block to reduce size  

* Save extra-large file in several machines
  - Master: matain metadata, map of chunk and chunk server
  - Slave: store chunk 
  - Master include index (chunks server) info, Salve ChunkServer includes chunk info  

* Write (client, server(master, slave))
  - 拆分写入DFS: 断点续传，以chunk为单位
  - client -> master: 写chunk1 
    master -> client: 回chunkserver list
    client -> slave: 写， 从slave1，2，3中选个队长1写进去，然后slave1再写给其他slave
  - Client: Database, WebServer, Server: GFS, client and server是相对的概念

* Update
  - write new data to fs, change the index in master to the new data address. Leave the old data there.

* Read
  - 拆分的部分从master index(chunk list)中知道，然后共同读取

* Scale
  + one master(90% cases) 不够
  + chunk坏了
    - checksum(MD5, SHA1, SHA256, SHA512), add checksum to the end of chunk. When read data, calculate checksum and compare the one at the end.
    - 周期性检查，读的时候检查checksum
    - Recover: ask master
  + Replica
    - 3份
  + heartbeat,验证chunkserver在不在

------------------------------------
Consistent Hashing & Design Tiny Url
------------------------------------
* Consistent Hashing by Horizontal Sharding
  - mod 360, 分配给n台机器，每个机器负责一段区间。n从2到3，只有1/3的数据移动。 
  - 目的是减小数据的移动
  - 缺陷：数据不均匀(e.g. n从3到4)，迁移压力大

* Consistent hashing 
  - hash func(数据) return (mod 2^64-1) 个点中一个， 把机器和数据放在这些点上。数据算好点后，顺时针找到的第一个机器(虚拟节点)为要存的。
  - 1个机器分成1000个虚拟节点，平均分布在环上。1000个虚拟节点有1000个不连续的区间

* Back up vs Replica
  - backup 是周期的，不是实时的
  - replica是实时的

* MySql Replica: Master - Slave
  - Master: write and read
  - Slave: read, sync from Master (Write Ahead Log, 执行master之前的操作，所以造成一定延迟)
  - Master 挂了，promote Slave.没有同步过来的数据可能丢失。

* NoSQL Replica
  - Consistent Hashing 环上顺时针存3份
  - include sharding and replica (优点)

* Design Tiny URL

* Scenario: 功能，需求，QPS，存储容量
  - redirect
  - 产生，点击Tiny URL， 存储量

* Servie
* Storage
  - transaction？ no
  - 丰富的query？a little bit
  - scalability? 
  - QPS?
  - Sequential id?

* Long URL to 6 digit url 
  - 随机转化：随机生成短的没有用过的url存数据库。随着短网址越来越多，生成速度效率低。
  - 进制转换：依赖全局自动增加iD，转化成6位62进制的short url。62^6=57B=570亿

* Scale
  - cache
  - 地理位置信息提速：DNS解析位置
  - honrizontal sharding: what sharding key? 
    1. Long URL 
       long  -> short: easy 
       short -> long: broadcast all database
    2. ID 
       short -> long: short url to ID, use ID to find db
       long  -> short: broadcast to all servers



















































































